% !TEX root =  paper.tex

\section{Empirical Evaluation}\label{sec:evaluation}

We consider the following research questions:

\noindent
%\textbf{RQ\textsubscript{1} (detection):} How effective is our proposed visual-aided approach in visually verifying test statements?
%\textbf{RQ\textsubscript{1} (prevalence):} How prevalent are test breakages in practice?

\noindent
\textbf{RQ\textsubscript{1} (repair):} How do DOM-based and visual-augmented test repair approaches compare in terms of effectiveness?

\noindent
\textbf{RQ\textsubscript{2} (running time):} What is the running time of executing our visual-augmented approach as compared to the DOM-based one? \\

%\noindent
%\textbf{RQ\textsubscript{4} (duration):} Does \tool decrease the duration of web test repair tasks?
%
%\noindent
%\textbf{RQ\textsubscript{5} (accuracy):} Does \tool increase the accuracy of web test repair tasks? \\

\noindent
The repair strategy of \tool can be customized so as to utilize: (i)~only DOM-based information, (ii)~only visual-based information,  (iii)~both DOM-based and visual information. We refer to these three variants as \tool-DOM, \tool-Vis, and \tool-Hybrid, respectively.
%In our study, the visual-augmented approach is represented by \tool, and the DOM-based approach is represented by \water. 
%RQ\textsubscript{1} aims at evaluating how effective the three variants of \tool are at detecting and repairing test breakages. 
% the correctness of the test statements or signalling the occurrence of a breakage, and how its effectiveness varies across breakage classes. 
%RQ\textsubscript{1} and RQ\textsubscript{2} aim at comparing our proposal against the state of the art web testing repair solution under different effectiveness and efficiency measures. Finally, we aim at evaluating whether the tool can effectively aid humans in detecting and repairing test breakages, as compared to a manual fashion.
Thus, RQ\textsubscript{1} and RQ\textsubscript{2} aim at comparing the effectiveness and efficiency of the three variants of our approach.

\subsection{Procedure}\label{sec:procedure}

Then, for each subject applications, we applied the three variants of our approach to each $(T_n,V_{n+1})$ pair in which either a breakage or a failure has been observed. For each variant, we ran the repaired test $T_{rep}$ to determined whether the tool was able to detect (and possibly repair) the issues. If $T_{rep}$ executed and passed, we counted the number of breakages that were \textit{corrected} (true positives). If $T_n$ failed because of a bug, and $T_{rep}$ was able to report it, we counted the number of correct \textit{detections} (true negatives). Conversely, the false negatives are represented by \textit{missed} detections or \textit{incorrect} repairs. False positives occur when our approach signalled a breakage or a failure, even if the statement is actually correct.


\subsection{Threats to Validity}\label{sec:ttv}

\head{Internal validity} We limited the bias of having produced test suites ourselves, that could constitute a threat to the internal validity of our work, by choosing existing test suites used in the previous web testing research. This also ensures, to some extent, that the chosen object of analysis are non-trivial, therefore representative of a test suites that a real web tester would implement. 

\head{External validity} Concerning the generalizability of our results, we ran our approach with a limited number of subjects and test suites. However, we believe the approach to be applicable in a general web testing scenario, even though the magnitude of the results might change when considering different test suites and applications. To limit biases in the manual selection of the versions, we considered \textit{all} the available releases after those for which the test suites were developed for.
 

%\textit{Procedure and Metrics.}
%
%For each considered application: 
%
%\begin{itemize}
%\item we executed each test suite $T_k$ created for a version $V_k$ by means of the Visual Trace Generator, which collected information about the tests execution (e.g., DOMs, screenshots, etc);
%\item we ran each $T_k$ on the next version $V_{k+1}$ with the Visual Test Suite Runner and collected information about each detection, breakage, automatic as well as manual repair triggered.
%\end{itemize}
%
%\noindent
%To answer RQ\textsubscript{1} (\textbf{detection}), we counted the number of statements that were correct, and that our technique was able to visually verify. Those cases represent the true positives (TP\textsubscript{detection}). On the other hand, if our tool deemed those cases as incorrect (i.e., was not able to verify them), then they represent the false negative FN\textsubscript{detection}. Conversely, if the statements were broken, and our technique detected the breakages, we counted them as true negative TN\textsubscript{detection}, whereas if the tool deemed them as correct, then is a false positive FP\textsubscript{detection}.
%%
%In the case of FN and TN, \tool attempts a repair. We also counted the number of correct (REP\textsubscript{correct}) and wrong repairs (REP\textsubscript{wrong}). (Note that in the FN cases, repairs were unnecessary (REP\textsubscript{unnecessary})). Moreover, to fully evaluate the limitations of our technique, we counted the statements that were repaired manually (REP\textsubscript{manual}) and the statements that were found to be unrepairable both by manually and automatically (REP\textsubscript{unrepairable}).
%
%\noindent
%To answer RQ\textsubscript{2}, we counted the number of tests that \tool and \water were able to repair.
%
%\subsection{Study 2}
%
%We designed a user study to measure how effective the tool is in the identification of the root cause, and how much time it allows to save.

\subsection{Results}\label{sec:results}











