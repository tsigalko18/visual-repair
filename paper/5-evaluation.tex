% !TEX root =  paper.tex

\section{Empirical Evaluation}\label{sec:evaluation}

%\subsection{Research Questions}

We consider the following research questions: \\
\noindent
\textbf{RQ\textsubscript{1} (effectiveness):} 
How effective is the proposed visual-aided approach at repairing test breakages as opposed to a state of the art DOM-based approach?
%How do DOM-based and visual-augmented test repair approaches compare in terms of effectiveness?

\noindent
\textbf{RQ\textsubscript{2} (performance):} What is the the overhead and the running time of executing our visual-aided approach as compared to the DOM-based one?

RQ\textsubscript{1} aims at evaluating how effective \tool is at detecting and repairing test breakages, and how its effectiveness varies across the breakage classes. 
RQ\textsubscript{2} aims at evaluating the overhead and running time imposed by the two main modules of \tool: the Visual Execution Tracer and the Visual-Augmented Test Runner.

%\subsection{Variables and Measures}

\input{results-table}

\head{Object of Analysis}
%
As objects of analysis we chose four (4) web applications and their test suites utilized in earlier study on web testing~\cite{WCRE}. For each application, we manually applied each test suite across all the available releases, collecting information about the breakages. At the end of the task, we identified 733 individual breakages (see details in \autoref{sec:taxonomy}).

\head{Independent Variables}
In our study, the visual-aided approach is represented by \tool. As a DOM-based approach we chose \water. However, we have not adopted the implementation described in the original paper~\cite{Choudhary:2011:WWA:2002931.2002935}. In fact, there are fundamental design differences between the algorithms: \water is an offline technique that runs a test, collects information about the breakages, and runs the repair as a \textit{post-processing} technique. Our algorithm, instead, is designed to analyze the test suite at runtime, and attempt repairs in a \textit{online} mode. %Thus, a straightforward comparison is not possible, as it would bias either of the two competing techniques.
Given that the scope of this paper is to compare the effectiveness in repairing breakages, we injected \water's DOM-based repair strategy within our main algorithm. Specifically, we customized the \textsc{visualSearch} procedure so as to utilize: (i)~only DOM-based information, (ii)~only visual-based information. For simplicity, in the rest of the evaluation section, we refer to the two competing tools as \water and \tool, respectively.

\head{Dependent Variables}
We aim to gain insights on the effectiveness and efficiency of the test repair approaches.

\noindent
\emph{Effectiveness}.
We evaluated our approach at repairing the 733 breakages found in the study presented in this paper in \autoref{sec:taxonomy}.
To this aim, we counted the number of \textit{correct repairs} and triggered by the two techniques, as well as, the amount of \textit{manual repairs}.

\noindent
\emph{Efficiency}.
We utilize two metrics of efficiency.
First, we measured the overhead imposed by the Visual Execution Tracer on the test suite to create the traces that underlie at the functioning of our approach.
As a second metric, we counted the time spent by each competing technique in attempting at repairing breakages. 

\head{Procedure}\label{sec:procedure}
For each subject applications, we applied \water and \tool to each $(T_n,V_{n+1})$ pair in which a breakage was observed. For each tool, and for each breakage, we manually examined the proposed repaired $rep$ to determined its correctness. If $rep$ was found correct upon manual inspection, we incremented the number of correct repairs (that represent our true positives). Conversely, the false negatives are represented by \textit{missed} detections or \textit{incorrect} repairs.
In the cases where both tools were unable to repair, we counted the number of \textit{manual} repairs.

%The manual inspection was necessary because the examined tests do have dependencies, and they often exhibit \textit{multiple} breakages.

\subsection{Results}\label{sec:results}

\head{Effectiveness (RQ\textsubscript{1})}
\autoref{table:results} presents the effectiveness results. 
For each web application, the table reports the number of breakages, and the amount of correct repairs triggered by \water and \tool both numerically and percentage-wise. 
The results are further divided among the various breakage classes. Totals across all applications are also provided.

Overall, we can notice that \water was able to repair 420/733 (57\%), whereas \tool found correct repairs for 592/733 (81\%) breakages. \tool was hence able to correct 172 breakages more than \water, a 41\% increment.

Looking at the specific breakage classes, each tool performed well with respect to \textit{Scenario~1}: \water repaired correctly 63\% of the times, whereas \tool 84\%. Concerning \textit{Scenario~2}, no correct repairs were found by \water, while \tool was successful 33\% of the times.
About elements being removed (\textit{Scenario~3}) both tools performed equally, detecting 14/21 cases (67\%). The main difference between the two approaches emerges when considering \textit{Mis-Selection} cases (\textit{Scenarios~4 and~5}): \water was never able to detect any mis-selection of elements, whereas \tool detected and repaired correctly on average 80\% of them (avoiding 94\% of direct breakages due to mis-selections and  preventing 67\% of propagated breakages).

\begin{table}[t]
\setlength{\tabcolsep}{1pt}
\renewcommand{\arraystretch}{0.9}
\centering
\caption{Comparison between DOM and Visual Repair Strategies for all breakages, and amount of Manual Repairs.}
\label{table:comparison}
\begin{tabular}{lSSSSS}
\toprule
& {\sc \small AddressBook} & {\sc \small Claroline} & {\sc \small Collabtive} & {\sc \small PPMA} & {\sc \small All Apps} \\
\midrule
\water\textsubscript{wins}	& 0           & 3         & 0          & 0    & 3        \\
\tool\textsubscript{wins} 	& 28          & 72        & 43         & 32   & 175      \\
Tie 	& 17          & 47        & 166        & 187  & 417      \\
Manual 	& 5           & 43        & 9          & 81   & 138      \\
\midrule
		& 50     & 165  & 218   & 300 & 733   \\
\bottomrule  
\end{tabular}
\end{table}

Overall, the proposed visual approach by \tool was constantly superior to \water, no matter the application being considered. The biggest improvement was for Claroline, with 69 (119-50) repairs more, whereas the lowest occurred for AddressBook, with ``only'' 32 (45-17) repairs more. 

\autoref{table:comparison} compares the two approaches further. We classified the repairs pertaining to each individual breakage in four categories: (1)~\water\textsubscript{wins}, meaning that \water was able to repair correctly whereas \tool failed; (2)~\tool\textsubscript{wins}, meaning that \tool prevailed; (3)~\textit{Tie}, when both variants were able to correct the breakage, and (4)~\textit{Manual}, when none of the variants were able to correct the breakage, and thus we had to manually fix the test to continue the experimental work. As a further note, no breakages were found to be unrepairable, or tests needed to be discarded. 

As evident from \autoref{table:comparison}, in most cases, both techniques found the correct repair (56\%). However, there are 175 cases in which the \tool repaired a breakage that \water could not fix. Conversely, only in a few cases (3) \water prevailed over the visual approach. Finally, 138/733 (19\%) of breakages were repaired \textit{manually}, because none of the technique were able to fix them.
We will discuss on those cases in \autoref{sec:discussion}.


\begin{table}[t]
\setlength{\tabcolsep}{0.9pt}
\renewcommand{\arraystretch}{0.9}
\centering
\caption{Performance results}
\label{table:performance}
\begin{tabular}{l@{\hskip 0.5em}SSS@{\hskip 1em}rr}
\toprule
& \multicolumn{3}{c}{\sc Trace Generation}
& \multicolumn{2}{c}{\sc Repair Time}      \\

\cmidrule(r){2-4} \cmidrule(r){5-6}

& \rot{{Original (s)}} 
& \rot{{With VET (s)}}
& \rot{Slowdown} 
& \rot{{\water (s)}} 
& \rot{{\tool (s)}}    \\

\midrule
\sc AddressBook & 49.65            & 207.03  & 4.2X    & 959    & 447  \\
\sc Claroline   & 154.93           & 860.55  & 5.6X    & 6,492  & 1,359 \\
\sc Collabtive  & 621.89           & 1780.20 & 2.9X    & 2,551  & 1,995 \\
\sc PPMA        & 227.45           & 457.80  & 2.0X    & 3,762  & 1,293 \\
\midrule
Total & 1053.91 & 3305.59 & 3.7X & 13,763 & 5,094 \\
%Average & 263.48 & 826.40 & 3.650 & 3441 & 1273 \\
\bottomrule
\end{tabular}
\end{table}

\head{Performance (RQ\textsubscript{2})}\label{sec:performance}
To assess the performance of running our approach, we measured the execution time on a macOS machine, equipped with a 2.3GHz Intel Core i7 and 16 GB of memory.
\autoref{table:performance} (Trace Generation) shows the total execution time, in seconds, of the correct version of all tests for which a breakage occurred, first disabling (Column~2) and then enabling (Column~3) the Visual Execution Tracer (VET) module. Column~4 shows the overhead on the test suite.
Overall, our technique imposed on average a 3.7X overhead on the tests execution ($\approx$38 min more than the original tests running time). The biggest overhead occurs for Collabtive ($\approx$19 min), whereas the lowest occurs for AddressBook ($\approx$3 min). 

Concerning the execution time of our repair technique, \autoref{table:performance} (Repair Time) shows the total running time, in seconds, for each tool. Overall, it is evident that \tool was almost three times faster than \water. The best case occurs for Claroline, where \tool employed 86 minutes less than \water to perform its analysis. The most marginal gain occurred for AddressBook and Collabtive, with `only' 9 minutes less. 
On average, across all systems \tool employed 6.95~s/breakage, whereas \water employed 18.78~s/breakage.

At a first sight, these results could seems counterintuitive (i.e., a search on the DOM \textit{should} be faster than an image processing pipeline). However, we will explain some of the reasons of such poor performance of the DOM-based version of the algorithm in the Discussion section.







