% !TEX root =  paper.tex
\section{Background}\label{sec:background}

%In this section, we present basic concepts
%about E2E test automation that are needed 
%to understand the remainder of the paper.
%We provide background information on 
%DOM-based and visual web testing approaches.
%To present these approaches and concepts
%we use the example provided in~\autoref{fig:ab-back}: 
%a simplified version of the AddressBook web application, 
%one of the subjects used in our study. 
%We consider a scenario in which a user 
%inserts a \texttt{username} and \texttt{password} 
%in the AddressBook login form 
%(\autoref{fig:ab-back-a}), 
%and if these credentials are correct, 
%the \texttt{username} (\texttt{`admin'}) is displayed on the top right corner of the homepage 
%(\autoref{fig:ab-back-b}).

\noindent
\textbf{Approaches to E2E Web Test Automation.}
In recent years, two major approaches to E2E web testing have emerged, each of them characterized by a diverse way of interacting with the AUT. 

\textit{DOM-based} tools access and inspect properties of the Document Object Model (DOM), the hierarchical structure underlying a web page. 
%To this category belong capture-replay (C\&R) and programmable tools. \textit{Capture-replay} tools are based on the recording of sequences of inputs and actions performed by the tester on the web application GUI. The recording process creates a test script (see~\autoref{t:seleniumtest}) that can be replayed in a unattended mode. 
%With \textit{programmable} tools, on the other hand, test cases themselves become software artefacts that developers write resorting to specific testing frameworks (see~\autoref{lst:login-no-abs-back}). 
Specific testing framework APIs support the creation of test scripts that automate the interaction with a web page and its elements, and the sequences of inputs and actions performed by the tester on the web application. A test script can, for instance, automatically fill-in and submit forms or click on hyperlinks. %(see \autoref{fig:ab-back-c}). 
There are many test automation tools for web applications available, e.g., Selenium~\cite{selenium}, Sahi~\cite{sahi}, and Ringer~\cite{ringer}. %A representative of the programmable category is Selenium WebDriver~\cite{selenium}, which is considered the flagship open-source test automation tool for web applications.

An emerging and relatively new approach is \textit{visual} web testing, in which the AUT is tested through its GUI. 
%Indeed, the emergence of new complex visual components in web pages has required new ways of interfacing with the web applications. 
Visual tools such as JAutomate~\cite{Alegroth2013jat}, Sikuli~\cite{Sikuli}, and EggPlant Functional~\cite{eggplant} use image recognition techniques to identify the web elements displayed on the web page. Visual web testing tools offer an interesting alternative, and they are increasingly being adopted also in industry~\cite{Alegroth2013jat}. 
%\autoref{fig:ab-back-d} shows the visual version of the DOM-based test of~\autoref{fig:ab-back-c}, developed using Sikuli~\cite{Sikuli}. 
%We can notice how web elements are localised by \textit{visual locators}, i.e., images representing a portion of the GUI.

\noindent
\textbf{The Dilemma.}
To date, both approaches coexist and are utilized. Since each category of testing tools come with advantages and disadvantages, it is not clear whether in the future one will eventually prevail over the other. Our insight behind this uncertain scenario is that the choice of the most adequate testing tool depends to a large extent on the characteristics of the AUT. 
Most of popular DOM-based tools (e.g., Selenium) do not take into account the GUI of the AUT, which is instead important because is typically used by the tester as the main oracle against which to evaluate the behaviour of the application. %
For instance, for highly-interactive web systems having complex visual components (e.g., Google Maps), the DOM can be complicated to retrieve, whereas relying on visual testing allows testers to more easily assert on the correctness of the web page visual content. Visual tools, however, completely abstract away the model of the page, and create actions and assertions in a purely visual manner. 

\noindent
\textbf{The Idea.}
We believe that the use of visual technologies in web testing can  be especially beneficial for regression testing purposes, rather than for test creation. For example, the GUI can be used to validate the correct execution of the tests as the AUT evolves over time (in a similar way as testers do), or to detect deviations from the correct behaviour.
Indeed, E2E tests are vastly used in \textit{regression} scenarios, i.e., to verify that the most recent code changes have not adversely affected existing features. To do so, already in place test cases are re-executed to ensure that the current functionalities still work correctly. 
%

\subsection{A Test Breakage Travelogue}\label{sec:breakage-travelogue}

E2E web tests are known for being very fragile in the face of software evolution~\cite{2016-leotta-Advances,2016-Leotta-JSEP,Hammoudi-2016-ICST}. %Indeed, automated test code is usually highly coupled with low-level implementation details such as HTML attributes and thus result fragile and difficult to maintain as the AUT evolves~\cite{2016-leotta-Advances,2016-Leotta-JSEP,Hammoudi-2016-ICST}. 
Even a minor change might break a previously developed test case, whose script would need to be repaired manually, or re-written to match the new version of the web application, even if conceptually the functionality is unaltered, and no errors are present in the application code.

\noindent
\textbf{Characterization of a breakage.}
In order to clarify the scope of our work, and avoid possible misinterpretations, it is hence important to emphasize the difference between test breakages and test failures. We consider a \textit{test breakage} as the event that occur when a test that was used to work and pass on a certain version $V$, fails to be applicable to a version $V+n$ ($n \geq 1$) due to changes in the application code that interrupt its execution unpremeditatedly. %This event arises, for instance, when the test no longer reflects the intended behavior of the web application.
This is different from cases when tests expose a program \textit{failure} and hence do something for which they have been designed (i.e., exposing regression faults).
Said that, in the context of this paper, we focus our analysis on test breakages.

\noindent
\textbf{Study of Breakages.}
In a recent study, researchers have categorized breakages happening as test suites for web applications are evolved~\cite{Hammoudi-2016-ICST}. 
%While the study considers C\&R test suites only, the taxonomy of breakages proposed by Hammoudi and colleagues offers interesting findings. 
%
Concerning the \textit{causal} characterization, web element \textit{locators} have emerged as the main cause of fragility (74\% of the totality of breakages). %, followed by problems with test values (e.g., assertion values or input data -- 15\%), page reloading (4\%), user sessions (2\%), and popup issues (5\%). 
%
This confirms previous anecdotal findings on the problem of fragile web element locators~\cite{2016-Leotta-JSEP,2014-leotta-WoSAR,Daniel:2011:AGR:2002931.2002937,2013-Ricca-wse}.
%
Indeed, the mapping between locators and web elements is heavily affected by changes to the web page layout/structure, that may render tests inapplicable. %, because the locators become ineffective, as thoroughly reported in the literature~\cite{2016-leotta-Advances,2016-Leotta-JSEP,Choudhary:2011:WWA:2002931.2002935,Hammoudi-2016-ICST,2013-Ricca-wse}. Indeed, locators that rely on properties of the DOM (e.g., HTML attributes or XPath expressions) are prone to break for a large variety of reasons such as attributes or nodes being removed/modified from the DOM~\cite{Choudhary:2011:WWA:2002931.2002935}.

Concerning the \textit{temporal} characterization of test breakages, there exist direct, propagated, and silent breakages~\cite{Hammoudi-2016-ICST}. A breakage is called \textit{direct} when the test stops at a statement $st_i$, and $st_i$ has to be repaired in order to let the test pass or continue its execution. With \textit{propagated} breakages, on the other hand, the test stops at a statement $st_i$, but another statement $st_j$, preceding $st_i$ (i.e., $i > j$), must be repaired in order to let the test pass or continue its execution. Finally, \textit{silent} breakages do not manifest explicitly because the test does not stop nor fail, but yet it diverges from its original intent, and only by manually checking its execution (for example by looking at the actions performed on the GUI), the tester can detect the mis-behaviour.

\noindent
\textbf{Existing Web Tests Repair Approaches.} Test repair techniques have been proposed in the last years~\cite{Gao:2016:SGT:3046547.3046580,Daniel:2011:AGR:2002931.2002937,Daniel:2009:RSR:1747491.1747538,Daniel:2010:TRU:1831708.1831734,Choudhary:2011:WWA:2002931.2002935,Hammoudi-2016-FSE}. 
To date, the state of the art web test repair algorithm is \water, by Choudhary et al.~\cite{Choudhary:2011:WWA:2002931.2002935}. 
%\water is based on differential testing, and compares the correct test execution with the failing one in order to find potential fixes for the broken tests. 
\water is a differential testing technique used to compare the executions of a test over two different releases, one where the test runs properly and another were it fails. By gathering data about these executions, \water examines the DOM-level differences between the two versions and uses heuristics to find and suggest potential repairs.
While the repair algorithm of \water has a straightforward design and can manage a good number of cases related to locators or assertions, it has a number of limitations that derive from its  DOM-related narrowness: First, this can lead to a great number of false positives, as recognized by the authors of the paper~\cite{Choudhary:2011:WWA:2002931.2002935}. Second, 
 relying only on the DOM may be insufficient to find candidate repairs. Third, the algorithm 
% does not reflect the way in which testers attempt at repair tests, an activity that often requires to inspect the GUI of the two applications in order to find the root causes of the breakages. Fourth, it 
 relies on the assumption that the repair has always to be triggered at the point in which the test stops, which makes it impossible to handle propagated breakages. 
 %In many of such cases, it is imperative for the tester to inspect the GUI.
 
\noindent
\textbf{Holistic root cause analysis.}
At this point, it should be clear that in order to do effective root cause analysis, the testers must take into consideration all aspects behind a breakage (e.g., its cause and its position in the test) and link them together to devise possible repairs that do not change the intended test scenario. This is why repairing web tests is often considered a burdensome activity that leads the test suites to be abandoned~\cite{Christophe2014}. In our experience, this is certainly due to the high frequency at which those tests break, but also due to the little tooling support in the identification of the root causes behind a breakage and its repair.
%
Let us focus on locator problems only. Indeed, despite repairing locators could seem mostly a mechanical and straightforward activity, it instead accounts for a number of challenging scenarios that make this activity extremely time-consuming and mine the applicability of existing repair techniques. 
%In the following of this section, we describe some of those most challenging scenarios. In~\autoref{sec:approach} we present our novel test breakage detection and repair technique for E2E web tests.
