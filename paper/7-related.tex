\section{Related Work}\label{sec:relwork}

\head{Web Test Repair} To our knowledge, the state of the art web test repair solution is \water~\cite{Choudhary:2011:WWA:2002931.2002935}, which we already discussed and evaluated. 
%\water is a differential testing technique used to compare the executions of a test over two different releases of a web application under test, one where the test runs properly and another were it fails. By gathering data about these executions, \water examines the DOM-level differences between the two versions and uses heuristics to find and suggest potential repairs.
%
A recent work~\cite{Hammoudi-2016-FSE} have adopted \water to repair the breakages happening to all the intermediate minor versions between two major releases of a web application. In this paper we presented and experimented with a novel test repair techniques based on a image processing pipeline. 
% and uses the \water heuristics to trigger the repairs. 
%While the number of code changes between two major releases is typically larger than the number of changes between any pair of successive releases or commits, it has to be said that those are also intertwined in manners that render repair heuristics less effective. Hence, applying a repair technique iteratively to intermediate versions or commits leaves the technique with fewer, less intertwined changes to consider per application, increasing its chances of success.
%Results of an empirical study comparing \textsc{WaterFall} to a coarse-grained approach (\water) show that the former is more effective than the latter at automatically repairing tests.


\head{Test Case Breakages}  
%\textit{Choudhary et al.}~\cite{Choudhary:2011:WWA:2002931.2002935} distinguish three main reasons for test scripts to break: (1)~structural, (2)~content, and (3)~blind. The first category concerns DOM elements that get repositioned in the tree due to node/attribute insertion, modification or deletion. The second category concerns the textual content that get updated during software evolution. At last, the third category is related to server-side changes that are not directly observable in any DOM modification, such as popup boxes.
A recent paper~\cite{Hammoudi-2016-ICST} present a taxonomy of web breakages in the context of record/replay tests. The authors classified the proximal causes of test breakages, that is, the reasons for which the tests break that are immediately observable by the testers (e.g., \texttt{id} not found). Being our focus on test repair, we present a \textit{temporal} characterization of the breakages happening to locators, which  more directly correlate and inspired the design of an automated test repair technique such as the one presented in this paper.

\head{Breakage Prevention} 
Recent papers have considered increasing the robustness and maintainability of web test suites. 
In order to make test scripts robust (i.e., trying to preventing them from breaking), several tools producing smart web element locators have been presented~\cite{2016-Leotta-JSEP,kartik:ase15,2015-leotta-ICST,Yandrapally:2014:RTA:2610384.2610390}
%the tools \textsc{ROBULA}~\cite{2014-leotta-WoSAR} and its successor \textsc{ROBULA+}~\cite{2016-Leotta-JSEP} are 
%capable of producing smart web element locators that have been empirically evaluated to be robust to minor GUI changes as compared with the state of the art tools. \textit{Bajaj et al.}~\cite{kartik:ase15} present LED, a tool that automatically synthesizes web element locators by solving a constraint satisfaction problem over the group of valid DOM states in a web application. To our knowledge, there have been only two papers that have presented techniques for repairing locator breakages in web tests. In addition, the multi-locator extension of ROBULA~\cite{2015-leotta-ICST} supports automated repair of broken locators by identifying and attempting to apply other potential locators generated by different tools, thus relying on a redundancy of set of locators. \textit{Yandrapally et al.}~\cite{Yandrapally:2014:RTA:2610384.2610390} address the problem of test script fragility in relation to locators, proposing approaches for robustly identifying UI elements by using contextual clues, which is also a form of prevention. Finally, \textit{Stocco et al.}~\cite{2017-Stocco-SQJ,2016-Stocco-ICWE,2016-Stocco-ICWE-demo,2015-Stocco-AST} investigate the automated generation of page objects that confine causes of test breakages to a single class, a form of breakage prevention. 

%At a high level, they distinguish between proximal and distal causes. They define a proximal cause of a breakage as the cause which is most closely related to that breakage. Distal causes, on the other hand, are the modification that are made on the web application during its natural evolution and/or maintenance. For instance, the fact that a developer removes a choice from a dropdown list is a distal cause; the fact that a test fails at localizing that missing element is the proximal cause for which a tester must find an appropriate repair. 
%
%In another paper, \textit{Leotta et al.}~\cite{2016-leotta-Advances} distinguish between \textit{structural} changes and \textit{logical} changes. The former involve the modification of the web page structure (i.e., the DOM) that impact only the low-level test script statement associated with that page and modification. The latter category, on the other hand, involve a change of the test scenario for which one or more test statements need to be added, altered or deleted.

\head{Visual GUI Testing}
Researchers have studied the use of visual GUI testing within automated test suites~\cite{Alegroth2017,Alegroth:2015:VGT:2780084.2780169,Alegroth:2016:MAT:2905389.2905646}. Their results confirm the challenges in verifying script correctness, which is deemed as a frustrating, costly and tedious task. 

\head{XBIs} 
Several tools have employed visual-based techniques to detect cross-browser incompatibilities~\cite{RoyChoudhary:2013:XAI:2486788.2486881,RoyChoudhary:2010:WAI:1912607.1913287,7102638}.
%;  The most notable are  
%X-PERT~\cite{RoyChoudhary:2013:XAI:2486788.2486881} employs both DOM and visual information to identify XBIs issues.
%WebDiff~\cite{RoyChoudhary:2010:WAI:1912607.1913287}
%WebSee~\cite{7102638} adopts image differencing techniques. 

\head{GUI Test Repair} 
\textit{Grechanik et al.}~\cite{Grechanik:2009:MEG:1555001.1555055} analyze an initial and modified GUI for differences and generate a report for engineers documenting ways in which test scripts may have broken.
\textit{Zhang et al.}~\cite{Zhang:2013:ARB:2483760.2483775} describe \textsc{FlowFixer}, a tool that repairs broken GUI workflows in desktop applications. 
%The tool needs as input an example of correct workflow, which must be demonstrated by the user. Information about the invoked methods are collected, and then matched in the new version of the application. 
%FlowFixer then instruments the new application, randomly executes GUI actions, and builds a map of the triggered methods. Based on the matched methods previously acquired, the tool proposed fixes to align the old workflow with that of the new application.
%Repairing a GUI test script is more challenging than repairing a broken workflow~\cite{Zhang:2013:ARB:2483760.2483775}; 
%according to~\cite{Zhang:2013:ARB:2483760.2483775}, only comparing GUIs is insufficient to repair broken workflows. For that, we aim at joining DOM and visual aspects of the tests.
%Differently from~\cite{Zhang:2013:ARB:2483760.2483775}, we do not ask the user to demonstrate a scenario on the web app, we use existing test cases. 
\textit{Memon}~\cite{Memon:2008:ARE:1416563.1416564} presents a model of the event structure of a GUI and which is also used to determine the modifications as it evolves. 
%Then, tests are verified to be usable on the modified GUI model and if they are not, user-defined repairing transformations are utilized to repair them. 
In similar fashion, \textit{Gao et al.}~\cite{Gao:2016:SGT:3046547.3046580} present \textsc{SITAR}, a model-based repair algorithm of unusable GUI test scripts. 
%The tool creates abstract test cases and maps them to the model of the application (which is an EFG). The approach requires the tester to manually apply model-based transformations to correct the unusable test abstractions that are finally synthesized as new low-level test scripts. 
\textsc{ReAssert} is a tool to repair mainly assertions in unit tests for Java applications~\cite{Daniel:2011:AGR:2002931.2002937,Daniel:2011:RTR:1985793.1985978,Daniel:2009:RSR:1747491.1747538,Daniel:2010:TRU:1831708.1831734}. 
%The tool utilizes instrumentation and static and dynamic analysis of the test execution traces mainly for the purpose of repairing assertions. 
\textit{Huang et al.}~\cite{Huang:2010:RGT:1828417.1828465} describe a genetic algorithm to automatically repair GUI test suites by evolving new test cases that increase the test suite's coverage while avoiding infeasible sequences.

\head{Computer Vision to assist SE tasks} 
The use of computer vision techniques to assist common software-engineering tasks is getting popular in the recent years. One of the foundational works on computer vision applied to testing is by \textit{Chang} and colleagues. Their tool \textsc{Sikuli}~\cite{Sikuli} allows testers to write a visual test script that uses images to specify which GUI components to interact with and what visual feedback to be observed. Their work shows how this approach can facilitate a number of testing activites such as unit testing, regression testing, and test-driven development. \textit{Feng} et al.~\cite{Feng:2016:MTR:2970276.2970367} use a combination of visual analysis and NLP techniques to assist the inspection of crowdsourced test reports. 
%Spatial pyramid matching (SPM) is employed to measure the similarity of test report screenshots and then used to prioritize bug reports. 
Recently, \textit{Ramler and Ziebermayr}~\cite{2017-Ramler-ICSTW} have illustrated the use of visual test automation joint with computer vision techniques to test physical properties of a mechatronic system,  
 %In their study visual test scripts were used to specify the expected system-level behaviour without the need to intercept signals or to access internal states via test interfaces.
whereas \textit{K{\i}ra\c{c}} and colleagues~\cite{KIRAC2018266} have applied an image processing pipeline to automate for test oracle automation of visual output systems (digital TVs). 
