% !TEX root =  paper.tex
\section{Discussion}\label{sec:discussion} 

In this section, we discuss some of our evaluation findings, tool design decisions and limitations, as well as threats to validity of our study.

\head{Prevalence} Our empirical study confirmed the findings of previous studies about the fragility of E2E web tests when used for regression testing scenarios (\autoref{sec:taxonomy}). 
Although such tests did fail 94 times due to actual bugs, the number of breakages (733) is still predominant. 329/2672 tests of our benchmark (12\%) experienced at least one breakage that the test engineer must inspect, and find an appropriate repair for. 

\head{Automatic Repair} 
Our evaluation revealed that \tool can repair more than 80\% of the total number of breakages correctly. Therefore, we believe our algorithm and its heuristics to be accurate in the repair task.

We investigated why \emph{all} breakages were not repaired. We enumerate some of the main reasons next, which pertain to both the inherent nature of our subjects and tests and some of design choices of the two competing approaches. 

Our results highlighted that \water cannot repair any Mis-Selection breakage (\textit{Scenarios~4 and~5}). This is motivated by the intrinsic design of the DOM-based algorithm. Indeed, one of the repair heuristic of \water attempts to search the target web element by its XPath in the new evolved DOM. If an element is found, then \water returns a wrong element. \tool, on the other hand, does not trust \textit{a priori} DOM-based locators and validates them by means of their visual appearance. This is why our solution could detect and repair $\approx$83\% of the \textit{Mis-Selection} cases. 

For analogous reasons, the DOM-based repair strategy of \water was ineffective when searching elements with local crawling (\textit{Scenario~2}). \tool could detect nearly one third of such cases. The failing cases were mostly due to the retrieving of no elements (or wrong ones). In PPMA, a challenging case occurred: the desired element was hidden in a hoverable dropdown menu, whereas the crawler is designed to search for the web element in the neighbouring states. Moreover, our tool currently does not support the creation of general purpose statements, such as the ones that need input data. %This leaves room for improvement. 

Concerning the elements being removed (\textit{Scenario~3}), both techniques failed in seven cases because the crawling phase did retrieve a wrong element. 
\textit{Scenario~1} represents the most common class of breakage, and both the techniques perform quite well. However, \tool had a $33\%$ increase in the correct repair rate. Only in three cases (for Claroline application), \water was able to find a correct repair when \tool was not. This was due to `edit' buttons whose DOM attributes were stable across the two considered releases whereas their visual appearance did change making our visual technique ineffective.

Looking at the results on a per-application basis, AddressBook and Collabtive tests experienced breakages pertaining to all classes. 
\tool performed better than \water, with 165\% and 11\% increment on the number of correct repairs, respectively. 
For AddressBook, the DOM gets evolved quite frequently across the releases. Thus, for a tester would be challenging to find reliable attributes upon which to create stable web elements locators  (and hence robust test suites). Also the GUI of the web application gets evolved as changes in background colour and font size to make it more appealing and user-friendly. However, our approach demonstrated to be robust to both shifting and scaling (invariant to translation and scale transformation). 
%
There are five mis-selection breakages that could not be corrected by either of the two approaches. Those cases refer to again `edit' buttons whose position in the DOM and visual appearance did change substantially. 

Collabtive is the application in which both techniques have their best performance. Indeed, both the DOM and the GUI remained quite stable across the analyzed releases. The high number of breakages is explained by the numerous XPath locators used in the test suite, which make them fragile to minor DOM changes. However, such modifications at a DOM level did not jeopardize the applicability of \water.

For Claroline and PPMA web applications, almost the entirety of the breakages refers to \textit{Scenario~1}, with \tool being 134\% and 17\% more effective than the DOM-based repair, respectively. Those applications are the ones in which more manual repairs were needed. Claroline experienced both major DOM and GUI evolutions, whereas in the case of PPMA our technique had problems with timing issues (e.g., delays or page reloads that were needed to display the web elements on the GUI).

\head{Performance and Overhead} 
On average, \water was nearly three times slower than \tool mainly for two reasons: (1)~if the target element was still on the same state, and the heuristics failed to find it, the crawler was unnecessarily executed, and (2)~one of the heuristics searches for web elements having \textit{similar} XPaths. In our applications, the DOM size can be quite big, and \water is often forced to elaborate the similarity on dozens of elements. On the contrary, \tool adopted advanced image-processing algorithms, which requires one single template matching operation on images of reasonable size (i.e., screenshots of web pages).

On the whole, we believe that the potential benefits outweigh the foreseeable overhead given (1)~our results in the automated repairs, and (2)~the speed of our image processing pipeline. However, should strict time constraints apply, the pre-processing step (Visual Execution Tracer) can be carried out overnight.

Manually inspecting the application to find a candidate repair  and create a locator for \textit{each} broken statement can be in fact quite time-consuming. Our own experience in repairing tests, which we were required to do while devising \tool, corroborates the costliness of the task. For example, in AddressBook, one of the test for the search functionality fails 11 times when applied from version 4.02 to version 4.1.1: three elements gets removed (\textit{Scenario~3}) and seven mis-selections occur (\textit{Scenario~4}). \tool created the dynamic visual execution trace of the test in 22~s, and then it found correct repairs for all 11 breakages in $\approx$57~s. Thus, in this specific case, our technique can have prospect for success if the manual detection and repair performed by a human tester is lower than 80~s. However, empirical studies with human subjects are necessary to measure the costs associated with repairs. Before incurring in such expense, we evaluated whether out technique has prospect for success against the state of the art algorithm.

\head{Applications} 
\tool can be used by test engineers to validate and repair their E2E web test cases. Each statement of the tests is associated with their visual trace information. Hence, this can aid testers to perform more effective \textit{root cause analysis}.

Alternatively, our technique can play a role in \textit{automating software oracles}. A similar approach is implemented in Applitools~\cite{applitools}, where testers can manually inject visual checks at specific places of the test's execution. This has two drawbacks: (1)~the insertion of the check-points must be performed manually, (2)~this extra-code clutters the initial test code, with statements that do not pertain to the test scenario itself. On the contrary, our technique can be introduced smoothly in the testing team, because it requires no modification at the source code level. 

Finally, \tool can be utilized as a \textit{runtime monitoring}  technique for the detection of breakages tests. We have demonstrated that \tool goes beyond simple detection and can efficiently correctly most of the breakages at runtime, hence it is a first step towards automated \textit{self-repair} testing techniques. 

\head{Threats to Validity}\label{sec:ttv}
We limited the bias of having produced test suites ourselves, that could constitute a threat to the internal validity of our work, by choosing existing test suites used in the previous web testing research. This also ensures, to some extent, that the chosen object of analysis are non-trivial, therefore representative of a test suites that a real web tester would implement. 
%
Concerning the generalizability of our results, we ran our approach with a limited number of subjects and test suites. However, we believe the approach to be applicable in a general web testing scenario, even though the magnitude of the results might change when considering different test suites and applications. To limit biases in the manual selection of the versions, we considered \textit{all} the available releases after those for which the test suites were developed for.